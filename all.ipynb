{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re\n",
    "import gc\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer  \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取数据\n",
    "train_data = pd.read_csv('train_public.csv')\n",
    "test_data = pd.read_csv('test_public.csv')\n",
    "train_init_data = pd.read_csv('train_internet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 填补缺失值f系\n",
    "col = ['f0', 'f1', 'f2', 'f3', 'f4']\n",
    "x_col = ['f0', 'f1', 'f2', 'f3', 'f4', 'f5']\n",
    "\n",
    "imp = SimpleImputer(strategy = 'median')\n",
    "for c in col:\n",
    "    train_data[c] = imp.fit_transform(train_data[c].values.reshape(-1,1))\n",
    "    test_data[c] = imp.fit_transform(test_data[c].values.reshape(-1,1))\n",
    "for x_ in x_col:\n",
    "    train_init_data[x_] = imp.fit_transform(train_init_data[x_].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理特征work_year、class、work_type\n",
    "\n",
    "work_year_dict = {'< 1 year': 0,'1 year': 1,'2 years': 2,'3 years': 3,'4 years': 4,'5 years': 5,\n",
    "                  '6 years': 6,'7 years': 7,'8 years': 8,'9 years': 9,'10+ years': 10}\n",
    "train_data['work_year'] = train_data['work_year'].map(work_year_dict)\n",
    "test_data['work_year'] = test_data['work_year'].map(work_year_dict)\n",
    "train_init_data['work_year'] = train_init_data['work_year'].map(work_year_dict)\n",
    "train_data['work_year'] = train_data['work_year'].fillna(1)\n",
    "test_data['work_year'] = test_data['work_year'].fillna(1)\n",
    "train_init_data['work_year'] = train_init_data['work_year'].fillna(1)\n",
    "\n",
    "class_dict = {'A':1,'B':2,'C':3,'D':4,'E':5,'F':6,'G':7}\n",
    "train_data['class'] = train_data['class'].map(class_dict)\n",
    "test_data['class'] = test_data['class'].map(class_dict)\n",
    "train_init_data['class'] = train_init_data['class'].map(class_dict)\n",
    "\n",
    "work_type_dict = {'公务员':0,'其他':1,'工人':2,'工程师':3,'职员':4}\n",
    "train_init_data['work_type'] = train_init_data['work_type'].map(work_type_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# issue_date提取时间特征\n",
    "\n",
    "train_data['issue_date'] = pd.to_datetime(train_data['issue_date'])\n",
    "test_data['issue_date'] = pd.to_datetime(test_data['issue_date'])\n",
    "train_init_data['issue_date'] = pd.to_datetime(train_init_data['issue_date'])\n",
    "\n",
    "train_data['issue_date_year'] = train_data['issue_date'].dt.year\n",
    "test_data['issue_date_year'] = test_data['issue_date'].dt.year\n",
    "train_init_data['issue_date_year'] = train_init_data['issue_date'].dt.year\n",
    "train_data['issue_date_month'] = train_data['issue_date'].dt.month\n",
    "test_data['issue_date_month'] = test_data['issue_date'].dt.month\n",
    "train_init_data['issue_date_month'] = train_init_data['issue_date'].dt.month\n",
    "train_data['issue_date_dayofweek'] = train_data['issue_date'].dt.dayofweek\n",
    "test_data['issue_date_dayofweek'] = test_data['issue_date'].dt.dayofweek\n",
    "train_init_data['issue_date_dayofweek'] = train_init_data['issue_date'].dt.dayofweek\n",
    "\n",
    "train_data.drop('issue_date', axis = 1, inplace=True)\n",
    "test_data.drop('issue_date', axis = 1, inplace=True)\n",
    "train_init_data.drop('issue_date', axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理earlies_credit_mon特征\n",
    "\n",
    "def findDate(val):\n",
    "    fd = re.search('(\\d+-)', val)\n",
    "    if fd is None:\n",
    "        return '1-'+val\n",
    "    return val + '-01'\n",
    "\n",
    "train_data['earlies_credit_mon'] = pd.to_datetime(train_data['earlies_credit_mon'].map(findDate))\n",
    "test_data['earlies_credit_mon'] = pd.to_datetime(test_data['earlies_credit_mon'].map(findDate))\n",
    "train_init_data['earlies_credit_mon'] = pd.to_datetime(train_init_data['earlies_credit_mon'].map(findDate))\n",
    "\n",
    "train_data['earliesCreditMon'] = train_data['earlies_credit_mon'].dt.month\n",
    "test_data['earliesCreditMon'] = test_data['earlies_credit_mon'].dt.month\n",
    "train_data['earliesCreditYear'] = train_data['earlies_credit_mon'].dt.year\n",
    "test_data['earliesCreditYear'] = test_data['earlies_credit_mon'].dt.year\n",
    "train_init_data['earliesCreditMon'] = train_init_data['earlies_credit_mon'].dt.month\n",
    "\n",
    "train_data.drop('earlies_credit_mon', axis = 1, inplace=True)\n",
    "test_data.drop('earlies_credit_mon', axis = 1, inplace=True)\n",
    "train_init_data.drop('earlies_credit_mon', axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理特征  employer_type、industry\n",
    "\n",
    "cat_cols = ['employer_type', 'industry']\n",
    "for col in cat_cols:\n",
    "    lab = LabelEncoder().fit(train_data[col])\n",
    "    train_data[col] = lab.transform(train_data[col])\n",
    "    test_data[col] = lab.transform(test_data[col])\n",
    "    train_init_data[col] = lab.transform(train_init_data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对数据集缺失进行填充\n",
    "train_data['pub_dero_bankrup'] = train_data['pub_dero_bankrup'].fillna(method = 'ffill')\n",
    "test_data['pub_dero_bankrup'] = test_data['pub_dero_bankrup'].fillna(method = 'ffill')\n",
    "train_init_data['pub_dero_bankrup'] = train_init_data['pub_dero_bankrup'].fillna(method = 'ffill')\n",
    "train_init_data = train_init_data.dropna(subset = ['post_code', 'debt_loan_ratio', 'title'])\n",
    "train_init_data['recircle_u'] = train_init_data['recircle_u'].fillna(train_init_data['recircle_u'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造policy_code\n",
    "\n",
    "#构建train_data的特征\n",
    "train_data.drop('policy_code', axis = 1, inplace=True)\n",
    "scaler = MinMaxScaler()\n",
    "scaler_data = scaler.fit_transform(train_data)\n",
    "clf = KMeans(n_clusters=2, random_state=10)\n",
    "pre = clf.fit(scaler_data)\n",
    "train = pre.labels_\n",
    "\n",
    "#构建test_data的特征\n",
    "test_data.drop('policy_code', axis = 1, inplace=True)\n",
    "scaler = MinMaxScaler()\n",
    "scaler_data = scaler.fit_transform(test_data) \n",
    "clf = KMeans(n_clusters=2, random_state=10)\n",
    "pre = clf.fit(scaler_data)\n",
    "test = pre.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将kmeans预测的policy_code特征保存\n",
    "train = pd.DataFrame(train)\n",
    "train.to_csv('train.csv', index=False)\n",
    "\n",
    "test = pd.DataFrame(test)\n",
    "test.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建sub_class特征\n",
    "def Kmeans(data, num):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler_data = scaler.fit_transform(data.loc[data['class'] == num])\n",
    "    clf = KMeans(n_clusters=5, random_state=546789)\n",
    "    pre = clf.fit(scaler_data)\n",
    "    test = pre.labels_\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Kmeans(train_data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class1\n",
       "3         407\n",
       "1         380\n",
       "2         337\n",
       "0         307\n",
       "4         267\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(data, columns=['class1'])\n",
    "data.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过class的A~G特征分为七类，对此七类分别预测\n",
    "##################################### 构建train集的sub_class特征\n",
    "for i in range(1,8):\n",
    "    data = Kmeans(train_data, i)\n",
    "    if i == 1:\n",
    "        data = pd.Series(data, index= train_data.loc[train_data['class'] == 1].index)\n",
    "        s1 = data.map({0:'A1', 1:'A2',2:'A3',3:'A4',4:'A5'})\n",
    "    if i == 2:\n",
    "        data = pd.Series(data, index= train_data.loc[train_data['class'] == 2].index)\n",
    "        s2 = data.map({0:'B1',1:'B2',2:'B3',3:'B4',4:'B5'})\n",
    "    if i == 3:\n",
    "        data = pd.Series(data, index= train_data.loc[train_data['class'] == 3].index)\n",
    "        s3 = data.map({0:'C1',1:'C2',2:'C3',3:'C4',4:'C5'})\n",
    "    if i == 4:\n",
    "        data = pd.Series(data, index= train_data.loc[train_data['class'] == 4].index)\n",
    "        s4 = data.map({0:'D1',1:'D2',2:'D3',3:'D4',4:'D5'})\n",
    "    if i == 5:\n",
    "        data = pd.Series(data, index= train_data.loc[train_data['class'] == 5].index)\n",
    "        s5 = data.map({0:'E1',1:'E2',2:'E3',3:'E4',4:'E5'})\n",
    "    if i == 6:\n",
    "        data = pd.Series(data, index= train_data.loc[train_data['class'] == 6].index)\n",
    "        s6 = data.map({0:'F1',1:'F2',2:'F3',3:'F4',4:'F5'})\n",
    "    if i == 7:\n",
    "        data = pd.Series(data, index= train_data.loc[train_data['class'] == 7].index)\n",
    "        s7 = data.map({0:'G1',1:'G2',2:'G3',3:'G4',4:'G5'})\n",
    "# 合并并保存\n",
    "data_train = pd.concat([s1, s2, s3, s4, s5, s6, s7]).reset_index(drop=True)\n",
    "data_train.to_csv('train_sub_class.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### 构建train集的sub_class特征\n",
    "for i in range(1,8):\n",
    "    data = Kmeans(test_data, i)\n",
    "    if i == 1:\n",
    "        data = pd.Series(data, index= test_data.loc[test_data['class'] == 1].index)\n",
    "        s1 = data.map({0:'A1', 1:'A2',2:'A3',3:'A4',4:'A5'})\n",
    "    if i == 2:\n",
    "        data = pd.Series(data, index= test_data.loc[test_data['class'] == 2].index)\n",
    "        s2 = data.map({0:'B1',1:'B2',2:'B3',3:'B4',4:'B5'})\n",
    "    if i == 3:\n",
    "        data = pd.Series(data, index= test_data.loc[test_data['class'] == 3].index)\n",
    "        s3 = data.map({0:'C1',1:'C2',2:'C3',3:'C4',4:'C5'})\n",
    "    if i == 4:\n",
    "        data = pd.Series(data, index= test_data.loc[test_data['class'] == 4].index)\n",
    "        s4 = data.map({0:'D1',1:'D2',2:'D3',3:'D4',4:'D5'})\n",
    "    if i == 5:\n",
    "        data = pd.Series(data, index= test_data.loc[test_data['class'] == 5].index)\n",
    "        s5 = data.map({0:'E1',1:'E2',2:'E3',3:'E4',4:'E5'})\n",
    "    if i == 6:\n",
    "        data = pd.Series(data, index= test_data.loc[test_data['class'] == 6].index)\n",
    "        s6 = data.map({0:'F1',1:'F2',2:'F3',3:'F4',4:'F5'})\n",
    "    if i == 7:\n",
    "        data = pd.Series(data, index= test_data.loc[test_data['class'] == 7].index)\n",
    "        s7 = data.map({0:'G1',1:'G2',2:'G3',3:'G4',4:'G5'})\n",
    "# 合并并保存\n",
    "data_test = pd.concat([s1, s2, s3, s4, s5, s6, s7]).reset_index(drop=True)\n",
    "data_test.to_csv('test_sub_class.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#重新读取数据\n",
    "train_data = pd.read_csv('train_public.csv')\n",
    "test_data = pd.read_csv('test_public.csv')\n",
    "train_init_data = pd.read_csv('train_internet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改数据集里的policy_code\n",
    "tra = pd.read_csv('train.csv')\n",
    "tes = pd.read_csv('test.csv')\n",
    "\n",
    "train_data['policy_code'] = tra\n",
    "test_data['policy_code'] = tes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在训练集和测试集里加入sub_class\n",
    "train_sub = pd.read_csv('train_sub_class.csv')\n",
    "test_sub = pd.read_csv('test_sub_class.csv')\n",
    "\n",
    "train_data['sub_class'] = train_sub\n",
    "test_data['sub_class']  = test_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 采用均值填充\n",
    "train_data['work_year'] = train_data['work_year'].fillna(-1)\n",
    "test_data['work_year'] = test_data['work_year'].fillna(-1)\n",
    "train_init_data['work_year'] = train_init_data['work_year'].fillna(-1)\n",
    "\n",
    "col_fill = ['f0', 'f1', 'f2', 'f3', 'f4']\n",
    "imp = SimpleImputer(strategy = 'median')\n",
    "for c in col_fill:\n",
    "    train_init_data[c] = imp.fit_transform(train_init_data[c].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_year_dict = {'< 1 year': 0,'1 year': 1,'2 years': 2,'3 years': 3, '4 years': 4,'5 years': 5,\n",
    "                  '6 years': 6,'7 years': 7, '8 years': 8,'9 years': 9,'10+ years': 10}\n",
    "\n",
    "train_data['work_year'] = train_data['work_year'].map(work_year_dict)\n",
    "test_data['work_year'] = test_data['work_year'].map(work_year_dict)\n",
    "train_init_data['work_year'] = train_init_data['work_year'].map(work_year_dict)\n",
    "\n",
    "class_dict = {'A':1, 'B':2,'C':3, 'D':4,'E':5, 'F':6,'G':7}\n",
    "\n",
    "train_data['class'] = train_data['class'].map(class_dict)\n",
    "test_data['class'] = test_data['class'].map(class_dict)\n",
    "train_init_data['class'] = train_init_data['class'].map(class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日期转换\n",
    "train_data['issue_date'] = pd.to_datetime(train_data['issue_date'])\n",
    "test_data['issue_date'] = pd.to_datetime(test_data['issue_date'])\n",
    "train_init_data['issue_date'] = pd.to_datetime(train_init_data['issue_date'])\n",
    "\n",
    "train_data['issue_date_year'] = train_data['issue_date'].dt.year\n",
    "test_data['issue_date_year'] = test_data['issue_date'].dt.year\n",
    "train_init_data['issue_date_year'] = train_init_data['issue_date'].dt.year\n",
    "\n",
    "train_data['issue_date_month'] = train_data['issue_date'].dt.month\n",
    "test_data['issue_date_month'] = test_data['issue_date'].dt.month\n",
    "train_init_data['issue_date_month'] = train_init_data['issue_date'].dt.month\n",
    "\n",
    "train_data['issue_date_dayofweek'] = train_data['issue_date'].dt.dayofweek\n",
    "test_data['issue_date_dayofweek'] = test_data['issue_date'].dt.dayofweek\n",
    "train_init_data['issue_date_dayofweek'] = train_init_data['issue_date'].dt.dayofweek\n",
    "\n",
    "\n",
    "train_data.drop('issue_date', axis = 1, inplace=True)\n",
    "test_data.drop('issue_date', axis = 1, inplace=True)\n",
    "train_init_data.drop('issue_date', axis = 1, inplace=True)\n",
    "\n",
    "def findDate(val):\n",
    "    fd = re.search('(\\d+-)', val)\n",
    "    if fd is None:\n",
    "        return '1-'+val\n",
    "    return val + '-01'\n",
    "\n",
    "train_data['earlies_credit_mon'] = pd.to_datetime(train_data['earlies_credit_mon'].map(findDate))\n",
    "test_data['earlies_credit_mon'] = pd.to_datetime(test_data['earlies_credit_mon'].map(findDate))\n",
    "train_init_data['earlies_credit_mon'] = pd.to_datetime(train_init_data['earlies_credit_mon'].map(findDate))\n",
    "\n",
    "train_data['earliesCreditMon'] = train_data['earlies_credit_mon'].dt.month\n",
    "test_data['earliesCreditMon'] = test_data['earlies_credit_mon'].dt.month\n",
    "\n",
    "train_data['earliesCreditYear'] = train_data['earlies_credit_mon'].dt.year\n",
    "test_data['earliesCreditYear'] = test_data['earlies_credit_mon'].dt.year\n",
    "\n",
    "train_init_data['earliesCreditMon'] = train_init_data['earlies_credit_mon'].dt.month\n",
    "\n",
    "train_data.drop('earlies_credit_mon', axis = 1, inplace=True)\n",
    "test_data.drop('earlies_credit_mon', axis = 1, inplace=True)\n",
    "train_init_data.drop('earlies_credit_mon', axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 特征编码\n",
    "cat_cols = ['employer_type', 'industry', 'sub_class']\n",
    "for col in cat_cols:\n",
    "    lab = LabelEncoder().fit(train_data[col])\n",
    "    train_data[col] = lab.transform(train_data[col])\n",
    "    test_data[col] = lab.transform(test_data[col])\n",
    "    train_init_data[col] = lab.transform(train_init_data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 异常值处理\n",
    "train_init_data = train_init_data[train_init_data['total_loan'] <= 38000] \n",
    "train_init_data = train_init_data[train_init_data['debt_loan_ratio'] <= 43.34] \n",
    "train_init_data = train_init_data[train_init_data['house_exist'] <= 2] \n",
    "\n",
    "train_init_data.reset_index()\n",
    "\n",
    "train_data.drop('user_id', axis = 1, inplace=True)\n",
    "test_data.drop('user_id', axis = 1, inplace=True)\n",
    "train_init_data.drop('user_id', axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_data, test_data, train_init_data]:\n",
    "    for item in ['f0','f1','f2','f3','f4']:\n",
    "        df['industry_to_mean_' + item] = df.groupby(['industry'])[item].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 47)\n",
      "(5000, 46)\n",
      "(742813, 48)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(train_init_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_target_encoding_feats(train, test, train_init, features, target_feature, target_feature1, n_fold=10):\n",
    "    # Target编码\n",
    "    # -----------------------------------train\n",
    "    tg_feats = np.zeros((train.shape[0], len(features)))\n",
    "    kfold = StratifiedKFold(n_splits=n_fold, random_state=1024, shuffle=True)\n",
    "    for _, (train_index, val_index) in enumerate(kfold.split(train[features], train[target_feature])):\n",
    "        df_train, df_val = train.iloc[train_index], train.iloc[val_index]\n",
    "        for idx, feat in enumerate(features):\n",
    "            target_mean_dict = df_train.groupby(feat)[target_feature].mean()\n",
    "            df_val[f'{feat}_mean_target'] = df_val[feat].map(target_mean_dict)\n",
    "            tg_feats[val_index, idx] = df_val[f'{feat}_mean_target'].values\n",
    "\n",
    "    for idx, feature in enumerate(features):\n",
    "        train[f'{feature}_mean_target'] = tg_feats[:, idx]\n",
    "        \n",
    "    # ---------------------------------train_init\n",
    "    tg_feats = np.zeros((train_init.shape[0], len(features)))\n",
    "    for _, (train_index, val_index) in enumerate(kfold.split(train_init[features], train_init[target_feature1])):\n",
    "        df_train, df_val = train_init.iloc[train_index], train_init.iloc[val_index]\n",
    "        for idx, feat in enumerate(features):\n",
    "            target_mean_dict = df_train.groupby(feat)[target_feature1].mean()\n",
    "            df_val[f'{feat}_mean_target'] = df_val[feat].map(target_mean_dict)\n",
    "            tg_feats[val_index, idx] = df_val[f'{feat}_mean_target'].values\n",
    "\n",
    "    for idx, feature in enumerate(features):\n",
    "        train_init[f'{feature}_mean_target'] = tg_feats[:, idx]\n",
    "        \n",
    "    # -------------------------------------test\n",
    "    for feat in features:\n",
    "        target_mean_dict = train.groupby(feat)[target_feature].mean()\n",
    "        test[f'{feat}_mean_target'] = test[feat].map(target_mean_dict)\n",
    "\n",
    "    return train, test, train_init\n",
    "\n",
    "features = ['house_exist', 'debt_loan_ratio', 'industry', 'title']\n",
    "train_data, test_data, train_init_data = \\\n",
    "    gen_target_encoding_feats(train_data, test_data, train_init_data, features, 'isDefault', 'is_default', n_fold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造交叉特征\n",
    "train_data['post_code_to_mean_interst'] = train_data.groupby(['post_code'])['interest'].transform('mean')\n",
    "test_data['post_code_to_mean_interst'] = test_data.groupby(['post_code'])['interest'].transform('mean')\n",
    "train_init_data['post_code_to_mean_interst'] = train_init_data.groupby(['post_code'])['interest'].transform('mean')\n",
    "\n",
    "train_data['industry_mean_interest'] = train_data.groupby(['industry'])['interest'].transform('mean')\n",
    "test_data['industry_mean_interest'] = test_data.groupby(['industry'])['interest'].transform('mean')\n",
    "train_init_data['industry_mean_interest'] = train_init_data.groupby(['industry'])['interest'].transform('mean')\n",
    "\n",
    "train_data['employer_type_mean_interest'] = train_data.groupby(['employer_type'])['interest'].transform('mean')\n",
    "test_data['employer_type_mean_interest'] = test_data.groupby(['employer_type'])['interest'].transform('mean')\n",
    "train_init_data['employer_type_mean_interest'] = train_init_data.groupby(['employer_type'])['interest'].transform('mean')\n",
    "\n",
    "train_data['recircle_u_std_recircle_b'] = train_data.groupby(['recircle_u'])['recircle_b'].transform('std')\n",
    "test_data['recircle_u_std_recircle_b'] = test_data.groupby(['recircle_u'])['recircle_b'].transform('std')\n",
    "train_init_data['recircle_u_std_recircle_b'] = train_init_data.groupby(['recircle_u'])['recircle_b'].transform('std')\n",
    "\n",
    "train_data['early_return_remove_early_return_amount'] = train_data['early_return_amount'] / train_data['early_return']\n",
    "test_data['early_return_remove_early_return_amount'] = test_data['early_return_amount'] / test_data['early_return']\n",
    "train_init_data['early_return_remove_early_return_amount'] = \\\n",
    "    train_init_data['early_return_amount'] / train_init_data['early_return']\n",
    "\n",
    "# 将比值后存在inf的值设为 0\n",
    "inf1 = np.isinf(train_data['early_return_remove_early_return_amount'])\n",
    "train_data['early_return_remove_early_return_amount'][inf1] = 0\n",
    "inf2 = np.isinf(test_data['early_return_remove_early_return_amount'])\n",
    "test_data['early_return_remove_early_return_amount'][inf2] = 0\n",
    "inf3 = np.isinf(train_init_data['early_return_remove_early_return_amount'])\n",
    "train_init_data['early_return_remove_early_return_amount'][inf3] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 56)\n",
      "(5000, 55)\n",
      "(742813, 57)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(train_init_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 样本扩充\n",
    "column1 = set(train_data.columns)\n",
    "column2 = set(test_data.columns)\n",
    "column3 = set(train_init_data.columns)\n",
    "same_col = list(column1.intersection(column3))\n",
    "nosasme_col = list(column1.difference(column3))\n",
    "\n",
    "train_init_name_data = train_init_data[same_col].copy()\n",
    "for col in nosasme_col:\n",
    "    train_init_name_data[col] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6131516\ttest: 0.6132079\ttest1: 0.6125669\tbest: 0.6125669 (0)\ttotal: 144ms\tremaining: 9m 34s\n",
      "100:\tlearn: 0.2558248\ttest: 0.2558565\ttest1: 0.2838707\tbest: 0.2838558 (99)\ttotal: 816ms\tremaining: 31.5s\n",
      "200:\tlearn: 0.2025371\ttest: 0.2025632\ttest1: 0.2804053\tbest: 0.2789965 (171)\ttotal: 1.41s\tremaining: 26.6s\n",
      "Stopped by overfitting detector  (40 iterations wait)\n",
      "\n",
      "bestTest = 0.2789965295\n",
      "bestIteration = 171\n",
      "\n",
      "Shrink model to first 172 iterations.\n",
      "第1次过滤auc分数：0.915497\n",
      "0:\tlearn: 0.6128461\ttest: 0.6129046\ttest1: 0.6137123\tbest: 0.6137123 (0)\ttotal: 8.22ms\tremaining: 32.9s\n",
      "100:\tlearn: 0.2555057\ttest: 0.2555374\ttest1: 0.2977512\tbest: 0.2972134 (91)\ttotal: 576ms\tremaining: 22.3s\n",
      "200:\tlearn: 0.1987487\ttest: 0.1987746\ttest1: 0.2951716\tbest: 0.2946017 (162)\ttotal: 1.12s\tremaining: 21.2s\n",
      "Stopped by overfitting detector  (40 iterations wait)\n",
      "\n",
      "bestTest = 0.2946016588\n",
      "bestIteration = 162\n",
      "\n",
      "Shrink model to first 163 iterations.\n",
      "第2次过滤auc分数：0.900992\n",
      "0:\tlearn: 0.6133939\ttest: 0.6134516\ttest1: 0.6090661\tbest: 0.6090661 (0)\ttotal: 12.7ms\tremaining: 50.9s\n",
      "100:\tlearn: 0.2554332\ttest: 0.2554643\ttest1: 0.2768771\tbest: 0.2767791 (99)\ttotal: 591ms\tremaining: 22.8s\n",
      "Stopped by overfitting detector  (40 iterations wait)\n",
      "\n",
      "bestTest = 0.2730901857\n",
      "bestIteration = 159\n",
      "\n",
      "Shrink model to first 160 iterations.\n",
      "第3次过滤auc分数：0.911221\n",
      "0:\tlearn: 0.6123020\ttest: 0.6123605\ttest1: 0.6127721\tbest: 0.6127721 (0)\ttotal: 9.62ms\tremaining: 38.5s\n",
      "100:\tlearn: 0.2533832\ttest: 0.2534130\ttest1: 0.2925366\tbest: 0.2924632 (99)\ttotal: 607ms\tremaining: 23.4s\n",
      "200:\tlearn: 0.2008822\ttest: 0.2009069\ttest1: 0.2871210\tbest: 0.2869304 (195)\ttotal: 1.18s\tremaining: 22.2s\n",
      "300:\tlearn: 0.1632311\ttest: 0.1632548\ttest1: 0.2854390\tbest: 0.2843849 (277)\ttotal: 1.71s\tremaining: 21s\n",
      "Stopped by overfitting detector  (40 iterations wait)\n",
      "\n",
      "bestTest = 0.2843849137\n",
      "bestIteration = 277\n",
      "\n",
      "Shrink model to first 278 iterations.\n",
      "第4次过滤auc分数：0.913935\n",
      "0:\tlearn: 0.6114983\ttest: 0.6115579\ttest1: 0.6149794\tbest: 0.6149794 (0)\ttotal: 10.9ms\tremaining: 43.5s\n",
      "100:\tlearn: 0.2493402\ttest: 0.2493708\ttest1: 0.3235271\tbest: 0.3232342 (99)\ttotal: 560ms\tremaining: 21.6s\n",
      "Stopped by overfitting detector  (40 iterations wait)\n",
      "\n",
      "bestTest = 0.3218977779\n",
      "bestIteration = 121\n",
      "\n",
      "Shrink model to first 122 iterations.\n",
      "第5次过滤auc分数：0.890465\n",
      "0:\tlearn: 0.6123565\ttest: 0.6124155\ttest1: 0.6134327\tbest: 0.6134327 (0)\ttotal: 11.7ms\tremaining: 46.9s\n",
      "100:\tlearn: 0.2528084\ttest: 0.2528384\ttest1: 0.2961380\tbest: 0.2958057 (97)\ttotal: 624ms\tremaining: 24.1s\n",
      "200:\tlearn: 0.1992458\ttest: 0.1992711\ttest1: 0.2906285\tbest: 0.2906285 (200)\ttotal: 1.18s\tremaining: 22.3s\n",
      "Stopped by overfitting detector  (40 iterations wait)\n",
      "\n",
      "bestTest = 0.2906284854\n",
      "bestIteration = 200\n",
      "\n",
      "Shrink model to first 201 iterations.\n",
      "第6次过滤auc分数：0.900052\n",
      "0:\tlearn: 0.6112508\ttest: 0.6113088\ttest1: 0.6122532\tbest: 0.6122532 (0)\ttotal: 10.7ms\tremaining: 42.8s\n",
      "100:\tlearn: 0.2520991\ttest: 0.2521294\ttest1: 0.2968435\tbest: 0.2955491 (83)\ttotal: 641ms\tremaining: 24.7s\n",
      "200:\tlearn: 0.2008547\ttest: 0.2008804\ttest1: 0.2902306\tbest: 0.2897708 (169)\ttotal: 1.2s\tremaining: 22.7s\n",
      "Stopped by overfitting detector  (40 iterations wait)\n",
      "\n",
      "bestTest = 0.2895592528\n",
      "bestIteration = 204\n",
      "\n",
      "Shrink model to first 205 iterations.\n",
      "第7次过滤auc分数：0.908288\n",
      "0:\tlearn: 0.6104831\ttest: 0.6105484\ttest1: 0.6151697\tbest: 0.6151697 (0)\ttotal: 9.91ms\tremaining: 39.6s\n",
      "100:\tlearn: 0.2499361\ttest: 0.2499670\ttest1: 0.3316224\tbest: 0.3316224 (100)\ttotal: 614ms\tremaining: 23.7s\n",
      "200:\tlearn: 0.1962245\ttest: 0.1962491\ttest1: 0.3248189\tbest: 0.3243526 (194)\ttotal: 1.18s\tremaining: 22.3s\n",
      "Stopped by overfitting detector  (40 iterations wait)\n",
      "\n",
      "bestTest = 0.3243526126\n",
      "bestIteration = 194\n",
      "\n",
      "Shrink model to first 195 iterations.\n",
      "第8次过滤auc分数：0.883020\n",
      "0:\tlearn: 0.6096778\ttest: 0.6097379\ttest1: 0.6152315\tbest: 0.6152315 (0)\ttotal: 25.3ms\tremaining: 1m 41s\n",
      "100:\tlearn: 0.2463805\ttest: 0.2464097\ttest1: 0.3401070\tbest: 0.3399761 (99)\ttotal: 599ms\tremaining: 23.1s\n",
      "Stopped by overfitting detector  (40 iterations wait)\n",
      "\n",
      "bestTest = 0.3391152851\n",
      "bestIteration = 134\n",
      "\n",
      "Shrink model to first 135 iterations.\n",
      "第9次过滤auc分数：0.872992\n",
      "0:\tlearn: 0.6134852\ttest: 0.6135489\ttest1: 0.6069681\tbest: 0.6069681 (0)\ttotal: 12.7ms\tremaining: 50.6s\n",
      "100:\tlearn: 0.2556578\ttest: 0.2556894\ttest1: 0.2565080\tbest: 0.2560220 (80)\ttotal: 590ms\tremaining: 22.8s\n",
      "200:\tlearn: 0.2032519\ttest: 0.2032779\ttest1: 0.2500351\tbest: 0.2500351 (200)\ttotal: 1.2s\tremaining: 22.6s\n",
      "Stopped by overfitting detector  (40 iterations wait)\n",
      "\n",
      "bestTest = 0.2500351265\n",
      "bestIteration = 200\n",
      "\n",
      "Shrink model to first 201 iterations.\n",
      "第10次过滤auc分数：0.924110\n",
      "==============================\n",
      "过滤分数:0.902084\n",
      "过滤后的数据量: (75586, 56)\n"
     ]
    }
   ],
   "source": [
    "#过滤模型\n",
    "y= train_data['isDefault']\n",
    "oof_preds = np.zeros(train_data.shape[0])\n",
    "sub_preds = np.zeros(train_init_name_data.shape[0])\n",
    "feats = [f for f in train_data.columns if f not in ['loan_id','isDefault']]\n",
    "fold = KFold(n_splits=10, shuffle=True, random_state=546789)\n",
    "for n_fold, (trn_idx, val_idx) in enumerate(fold.split(train_data)):\n",
    "    trn_x, trn_y = train_data[feats].iloc[trn_idx], y.iloc[trn_idx]\n",
    "    val_x, val_y = train_data[feats].iloc[val_idx], y.iloc[val_idx]\n",
    "    clf = CatBoostClassifier(class_weights = [1,1.15],depth = 6,learning_rate = 0.08,iterations = 4000,\n",
    "                            bootstrap_type = 'Bernoulli',subsample = 0.9,random_seed = 546789,verbose = 0)\n",
    "    clf.fit(trn_x, trn_y, eval_set= [(trn_x, trn_y), (val_x, val_y)], verbose=100, early_stopping_rounds=40)\n",
    "    oof_preds[val_idx] = clf.predict_proba(val_x)[:, 1]\n",
    "    sub_preds += clf.predict_proba(train_init_name_data[feats])[:, 1] / fold.n_splits\n",
    "    print('第%d次过滤auc分数：%.6f' % (n_fold + 1, roc_auc_score(val_y, oof_preds[val_idx])))\n",
    "    del clf, trn_x, trn_y, val_x, val_y\n",
    "    gc.collect()\n",
    "train_init_name_data['isDefault'] = sub_preds\n",
    "IntePre = train_init_name_data[['loan_id', 'isDefault']]\n",
    "InteId = IntePre.loc[IntePre.isDefault<0.07, 'loan_id'].tolist()\n",
    "train = train_data\n",
    "test = test_data\n",
    "train_init = train_init_name_data\n",
    "train_init['isDefault'] = train_init_data['is_default']\n",
    "use_te = train_init[train_init.loan_id.isin(InteId)].copy()\n",
    "data = pd.concat([train,test,use_te]).reset_index(drop=True)\n",
    "print('='*30+'\\n过滤分数:%.6f\\n过滤后的数据量:' % (roc_auc_score(y, oof_preds)) ,data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75586, 56)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.97367\tvalidation_1-auc:0.97842\n",
      "[88]\tvalidation_0-auc:0.99316\tvalidation_1-auc:0.98783\n",
      "第1次auc分数：0.987867\n",
      "[0]\tvalidation_0-auc:0.98137\tvalidation_1-auc:0.97098\n",
      "[100]\tvalidation_0-auc:0.99413\tvalidation_1-auc:0.98518\n",
      "[116]\tvalidation_0-auc:0.99496\tvalidation_1-auc:0.98551\n",
      "第2次auc分数：0.985654\n",
      "[0]\tvalidation_0-auc:0.98303\tvalidation_1-auc:0.98533\n",
      "[100]\tvalidation_0-auc:0.99398\tvalidation_1-auc:0.98805\n",
      "[118]\tvalidation_0-auc:0.99488\tvalidation_1-auc:0.98803\n",
      "第3次auc分数：0.988116\n",
      "[0]\tvalidation_0-auc:0.97774\tvalidation_1-auc:0.98252\n",
      "[80]\tvalidation_0-auc:0.99286\tvalidation_1-auc:0.98859\n",
      "第4次auc分数：0.989050\n",
      "[0]\tvalidation_0-auc:0.98263\tvalidation_1-auc:0.98357\n",
      "[44]\tvalidation_0-auc:0.99107\tvalidation_1-auc:0.98564\n",
      "第5次auc分数：0.986039\n",
      "[0]\tvalidation_0-auc:0.97377\tvalidation_1-auc:0.97935\n",
      "[57]\tvalidation_0-auc:0.99184\tvalidation_1-auc:0.98685\n",
      "第6次auc分数：0.987011\n",
      "[0]\tvalidation_0-auc:0.97498\tvalidation_1-auc:0.96006\n",
      "[44]\tvalidation_0-auc:0.99168\tvalidation_1-auc:0.98445\n",
      "第7次auc分数：0.984532\n",
      "[0]\tvalidation_0-auc:0.98093\tvalidation_1-auc:0.97793\n",
      "[79]\tvalidation_0-auc:0.99291\tvalidation_1-auc:0.98766\n",
      "第8次auc分数：0.987874\n",
      "[0]\tvalidation_0-auc:0.97049\tvalidation_1-auc:0.95842\n",
      "[100]\tvalidation_0-auc:0.99386\tvalidation_1-auc:0.98675\n",
      "[177]\tvalidation_0-auc:0.99758\tvalidation_1-auc:0.98660\n",
      "第9次auc分数：0.986931\n",
      "[0]\tvalidation_0-auc:0.98191\tvalidation_1-auc:0.98159\n",
      "[55]\tvalidation_0-auc:0.99171\tvalidation_1-auc:0.98785\n",
      "第10次auc分数：0.988120\n",
      "==============================\n",
      "XGB单模线下分:0.937671\n"
     ]
    }
   ],
   "source": [
    "#重新定义新的数据集\n",
    "train = data[data['isDefault'].notna()]\n",
    "test  = data[data['isDefault'].isna()]\n",
    "y = train['isDefault']\n",
    "#xgboost单模\n",
    "oof_preds = np.zeros(train.shape[0])\n",
    "sub_preds = np.zeros(test.shape[0])\n",
    "feats = [f for f in train.columns if f not in ['loan_id','isDefault'] ]\n",
    "fold = KFold(n_splits=10, shuffle=True, random_state=1122)\n",
    "for n_fold, (trn_idx, val_idx) in enumerate(fold.split(train,y)):\n",
    "    trn_x, trn_y = train[feats].iloc[trn_idx], y.iloc[trn_idx]\n",
    "    val_x, val_y = train[feats].iloc[val_idx], y.iloc[val_idx]\n",
    "    clf = XGBClassifier(eval_metric='auc',max_depth=5, alpha=0.3, reg_lambda=0.3, subsample=0.8,\n",
    "                        colsample_bylevel = 0.867, objective='binary:logistic', use_label_encoder=False,\n",
    "                        learning_rate=0.08, n_estimators=4000, min_child_weight = 2, tree_method='hist',\n",
    "                        n_jobs=-1)\n",
    "    clf.fit(trn_x, trn_y, eval_set= [(trn_x, trn_y), (val_x, val_y)], verbose=100, early_stopping_rounds=40)\n",
    "    oof_preds[val_idx] = clf.predict_proba(val_x, ntree_limit=clf.best_ntree_limit)[:, 1]\n",
    "    sub_preds += clf.predict_proba(test[feats], ntree_limit=clf.best_ntree_limit)[:, 1]/fold.n_splits\n",
    "    print('第%d次auc分数：%.6f' % (n_fold + 1, roc_auc_score(val_y, oof_preds[val_idx])))\n",
    "    del clf, trn_x, trn_y, val_x, val_y\n",
    "    gc.collect() \n",
    "test['isDefault'] = sub_preds\n",
    "print('='*30+'\\nXGB单模线下分:%.6f' % roc_auc_score(y, oof_preds)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导出结果\n",
    "pre = test[['loan_id', 'isDefault']]\n",
    "pre.rename({'loan_id': 'id'}, axis=1)[['id', 'isDefault']].to_csv('result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(471, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('result.csv')\n",
    "data = data[data.isDefault > 0.5]\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'scipy' has no attribute 'norm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-b69d25962637>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'isDefault'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'scipy' has no attribute 'norm'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
